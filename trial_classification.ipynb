{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2\n", "import sys\n", "import os\n", "project_root = os.path.abspath(\"\")\n", "if project_root not in sys.path:\n", "    sys.path.append(project_root)\n", "import pandas as pd\n", "import numpy as np \n", "import utils.classification_utils\n", "from utils.classification_utils import *\n", "from pathlib import Path\n", "import harp\n", "import matplotlib.pyplot as plt\n", "import matplotlib.dates as mdates\n", "import matplotlib.ticker as mticker\n", "import aeon.io.video as video\n", "from ipywidgets import widgets\n", "from IPython.display import display\n", "import re\n", "import os\n", "import zoneinfo\n", "from datetime import datetime, timezone\n", "from functools import reduce\n", "from src.processing import detect_settings\n", "import src.processing.detect_stage as detect_stage_module\n", "\n", "%matplotlib widget"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Multi Date or SubjID Analysis. Can analyze all Sessions for given SubjID (run on subjid only), all SubjIDs for a date (run on date only), or specific SubjID(s) and Date(s) (run on lists of each)\n", "# To analyze all subjids for a date, or vice versa, set the other argument to None\n", "# For Dates: use lists for specific dates [YYYYMMDD], or use range(start_date, end_date) for a date range (inclusive)\n", "\n", "subjids = [40]\n", "dates = [20251205]\n", "\n", "multi_run_results = batch_analyze_sessions(subjids=subjids, dates=dates, save=True, verbose=False, print_summary=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["res = analyze_session_multi_run_by_id_date(38, 20251124, verbose=False, print_summary=True, save=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Loading experiments --> just define the SUBJID and DATE\n", "root = load_experiment(38, 20251121, index=2) #can add index for multiple experiments; index=0 as default\n", "stage = detect_stage_module.detect_stage(root)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["root = load_experiment(40, 20251127, index=0)\n", "time_window = ('15:46:30', '15:47:30')\n", "plot_valve_activity = plot_valve_and_poke_events(root=root, time_window=None)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = load_all_streams(root)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["events = load_experiment_events(root)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["odor_map = load_odor_mapping(root, data=data, verbose=True)\n", "print(\"Data streams loaded:\", list(data.keys()))\n", "print(\"Event types loaded:\", list(events.keys()))\n", "print(\"Odor mapping:\", odor_map['odour_to_olfactometer_map'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trial_counts = detect_trials(data, events, root, odor_map, verbose=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trial_outcomes_complete = classify_and_analyze_with_response_times(data, events, trial_counts, odor_map, stage, root)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Miscellaneous "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# compare recent res results to previously saved results\n", "from pathlib import Path\n", "import pandas as pd\n", "\n", "# Load old data\n", "json_path = Path(\"/Volumes/harris/hypnose/derivatives/sub-039_id-258/ses-012_date-20251112/saved_analysis_results\")\n", "df_old_csv_path = json_path / \"completed_sequences.csv\"\n", "\n", "if df_old_csv_path.exists():\n", "    df_old = pd.read_csv(df_old_csv_path)\n", "    print(f\"Loaded old data from: {df_old_csv_path}\")\n", "else:\n", "    print(f\"File not found: {df_old_csv_path}\")\n", "    df_old = pd.DataFrame()\n", "\n", "# Load new data\n", "cls = res['classification']\n", "df_new = cls['completed_sequences']\n", "\n", "# Print counts\n", "print(f\"\\nOld completed_sequences (from CSV): {len(df_old)} entries\")\n", "print(f\"New completed_sequences (from res): {len(df_new)} entries\")\n", "\n", "# Get unique initiation_sequence_time values\n", "old_times = set(pd.to_datetime(df_old['initiation_sequence_time'], errors='coerce').dropna())\n", "new_times = set(pd.to_datetime(df_new['initiation_sequence_time'], errors='coerce').dropna())\n", "\n", "# Find differences\n", "only_in_old = old_times - new_times\n", "only_in_new = new_times - old_times\n", "\n", "print(f\"\\n{'='*80}\")\n", "print(f\"Unique initiation_sequence_time in old but not in new: {len(only_in_old)}\")\n", "print(f\"{'='*80}\")\n", "if only_in_old:\n", "    old_mask = pd.to_datetime(df_old['initiation_sequence_time'], errors='coerce').isin(only_in_old)\n", "    pd.set_option('display.max_rows', None)\n", "    pd.set_option('display.max_columns', None)\n", "    pd.set_option('display.width', None)\n", "    pd.set_option('display.max_colwidth', None)\n", "    display(df_old[old_mask])\n", "else:\n", "    print(\"None\")\n", "\n", "print(f\"\\n{'='*80}\")\n", "print(f\"Unique initiation_sequence_time in new but not in old: {len(only_in_new)}\")\n", "print(f\"{'='*80}\")\n", "if only_in_new:\n", "    new_mask = pd.to_datetime(df_new['initiation_sequence_time'], errors='coerce').isin(only_in_new)\n", "    pd.set_option('display.max_rows', None)\n", "    pd.set_option('display.max_columns', None)\n", "    pd.set_option('display.width', None)\n", "    pd.set_option('display.max_colwidth', None)\n", "    display(df_new[new_mask])\n", "else:\n", "    print(\"None\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# find completed rows whose initiation_sequence_time is present (not-null)\n", "# but not present in completed_sequences_HR_missed\n", "cls = res.get('classification', {})\n", "\n", "completed = cls.get('completed_sequences', pd.DataFrame())\n", "hr_missed = cls.get('completed_sequences_HR_missed', cls.get('completed_sequences_hr_missed', pd.DataFrame()))\n", "\n", "if 'initiation_sequence_time' not in completed.columns:\n", "    print(\"completed_sequences has no 'initiation_sequence_time' column.\")\n", "else:\n", "    hr_times = hr_missed['initiation_sequence_time'] if 'initiation_sequence_time' in hr_missed.columns else pd.Series(dtype='datetime64[ns]')\n", "    hr_set = set(hr_times.dropna())\n", "\n", "    mask = completed['initiation_sequence_time'].notna() & ~completed['initiation_sequence_time'].isin(hr_set)\n", "    diff_df = completed[mask].copy()\n", "\n", "    print(f\"Rows in completed_sequences with initiation_sequence_time not in completed_sequences_HR_missed: {len(diff_df)}\")\n", "    display(diff_df)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Count completed_sequences with odor_sequence length < 5 (and > 5) and print them\n", "cls = res.get('classification', res)\n", "completed = cls.get('completed_sequences', pd.DataFrame())\n", "\n", "if 'odor_sequence' not in completed.columns:\n", "    print(\"completed_sequences has no 'odor_sequence' column.\")\n", "else:\n", "    def _to_list(seq):\n", "        if isinstance(seq, (list, tuple)):\n", "            return list(seq)\n", "        if pd.isna(seq):\n", "            return []\n", "        if isinstance(seq, str):\n", "            s = seq.strip()\n", "            try:\n", "                v = ast.literal_eval(s)\n", "                if isinstance(v, (list, tuple)):\n", "                    return list(v)\n", "            except Exception:\n", "                pass\n", "            s2 = s.strip(\"[]\")\n", "            parts = [p.strip().strip(\"'\\\"\") for p in s2.split(\",\") if p.strip()]\n", "            return parts\n", "        return []\n", "\n", "    seq_lens = completed['odor_sequence'].apply(lambda x: len(_to_list(x)))\n", "    short_mask = seq_lens < 5\n", "    long_mask = seq_lens > 5\n", "\n", "    short_df = completed[short_mask].copy()\n", "    long_df = completed[long_mask].copy()\n", "\n", "    print(f\"Completed sequences with odor_sequence length < 5: {len(short_df)}\")\n", "    if not short_df.empty:\n", "        display(short_df)\n", "\n", "    print(f\"Completed sequences with odor_sequence length > 5: {len(long_df)}\")\n", "    if not long_df.empty:\n", "        display(long_df)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cls = res['classification']\n", "# Code like this can be used to extract data from res. This will display all trials with < threshold poke times in completed sequences\n", "comp = cls[\"completed_sequences_with_response_times\"]\n", "pos_pokes_all = build_position_pokes_table(cls)\n", "short_pokes = build_position_pokes_table(cls, threshold_ms=200)\n", "display(short_pokes) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Find X s window with the most rewarded trials - used to find video segments\n", "window_sec = 40\n", "def find_peak_rewarded_window(res, window_sec=40):\n", "    # Get rewarded trials table\n", "    cls = res.get(\"classification\", res)\n", "    df = cls.get(\"completed_sequence_rewarded\", pd.DataFrame())\n", "    if df.empty:\n", "        print(\"No rewarded trials found.\")\n", "        return None\n", "\n", "    # Use valve_open_ts as trial time (or poke_first_in if you prefer)\n", "    times = pd.to_datetime(df[\"sequence_start\"], errors=\"coerce\")\n", "    df = df.assign(trial_time=times)\n", "    df = df.dropna(subset=[\"trial_time\"]).sort_values(\"trial_time\").reset_index(drop=True)\n", "\n", "    # Find the window with the most rewarded trials\n", "    best_count = 0\n", "    best_start = None\n", "    best_end = None\n", "    best_indices = []\n", "\n", "    trial_times = df[\"trial_time\"].values\n", "    n = len(trial_times)\n", "    for i in range(n):\n", "        start = trial_times[i]\n", "        end = start + np.timedelta64(window_sec, \"s\")\n", "        # Find all trials within [start, end)\n", "        mask = (trial_times >= start) & (trial_times < end)\n", "        count = mask.sum()\n", "        if count > best_count:\n", "            best_count = count\n", "            best_start = start\n", "            best_end = end\n", "            best_indices = np.where(mask)[0]\n", "\n", "    print(f\"Max rewarded trials in any {window_sec}s window: {best_count}\")\n", "    print(f\"Window: {best_start} to {best_end}\")\n", "    # Optionally display the trials in that window\n", "    display(df.iloc[best_indices])\n", "    return df.iloc[best_indices]\n", "\n", "peak_window_trials = find_peak_rewarded_window(res, window_sec=window_sec)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cutting video segments around start and end times. E.g., used to get video around peak rewarded trials identified above. \n", "# Automatically finds correct file, but runs faster if index is specified\n", "# specify 30 or 60 fps (fps mismatch visible if video duration is not matching input duration)\n", "start_time = \"14:34:29\"\n", "end_time = \"14:35:46\"\n", "cut_video(38, 20251119, start_time, end_time, fps=60, show_odor_overlay=True)#, vertical_text=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Testing of new functions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Debugging Functions:\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Quick code to look for brief purge events in between odors in completed trials\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "from bisect import bisect_left, bisect_right\n", "\n", "def list_short_purge_between_odors(trial_outcomes_complete, odor_map, threshold_ms=200.0, onset_slack_ms=50.0, verbose=True):\n", "    \"\"\"\n", "    Find Purge events shorter than threshold_ms that occur between distinct odor presentations\n", "    in completed trials. A Purge onset is counted if it lies within:\n", "       [current_odor_end - onset_slack_ms, next_odor_start + onset_slack_ms]\n", "    Returns a list of dicts (one per event) and prints a summary.\n", "    \"\"\"\n", "\n", "    # --- Helpers to resolve Purge columns ---\n", "    def _ci_get(d, key):\n", "        if not isinstance(d, dict):\n", "            return None\n", "        lk = str(key).lower()\n", "        for k, v in d.items():\n", "            if str(k).lower() == lk:\n", "                return v\n", "        return None\n", "\n", "    def _col_to_idx(col, row_len=None):\n", "        if isinstance(col, (int, np.integer)):\n", "            idx_num = int(col)\n", "        else:\n", "            m = re.search(r'(\\d+)', str(col))\n", "            if not m:\n", "                return None\n", "            idx_num = int(m.group(1))\n", "        if row_len is None:\n", "            return idx_num\n", "        if 0 <= idx_num < row_len:\n", "            return idx_num\n", "        if 1 <= idx_num <= row_len:\n", "            return idx_num - 1\n", "        return None\n", "\n", "    def _resolve_odor_name(odor_map, olf_id, idx, col=None):\n", "        v2o = odor_map.get(\"valve_to_odor\", {})\n", "        if isinstance(v2o, dict):\n", "            name = v2o.get((olf_id, idx))\n", "            if name is None and col is not None:\n", "                name = v2o.get(col)\n", "            if name is None:\n", "                name = v2o.get(idx)\n", "            if isinstance(name, str):\n", "                return name\n", "        grid = odor_map.get(\"odour_to_olfactometer_map\") or odor_map.get(\"odor_to_olfactometer_map\")\n", "        if isinstance(grid, (list, tuple)) and len(grid) > olf_id and isinstance(grid[olf_id], (list, tuple)):\n", "            row = grid[olf_id]\n", "            if 0 <= idx < len(row):\n", "                return row[idx]\n", "        return None\n", "\n", "    def _purge_columns(odor_map):\n", "        cols = []\n", "        olf_valves = odor_map[\"olfactometer_valves\"]\n", "        grid = odor_map.get(\"odour_to_olfactometer_map\") or odor_map.get(\"odor_to_olfactometer_map\")\n", "        for olf_id, df in olf_valves.items():\n", "            if df is None or getattr(df, \"empty\", True):\n", "                continue\n", "            row_len = len(grid[olf_id]) if isinstance(grid, (list, tuple)) and len(grid) > olf_id else None\n", "            for col in df.columns:\n", "                idx = _col_to_idx(col, row_len=row_len)\n", "                if idx is None:\n", "                    continue\n", "                name = _resolve_odor_name(odor_map, olf_id, idx, col=col)\n", "                if isinstance(name, str) and name.lower() == \"purge\":\n", "                    cols.append((olf_id, idx, col))\n", "        return cols\n", "\n", "    def _purge_events(odor_map):\n", "        \"\"\"Return list of dicts: {start,end,duration_ms,olf_id,idx,col} across all purge columns.\"\"\"\n", "        evs = []\n", "        for olf_id, idx, col in _purge_columns(odor_map):\n", "            df = odor_map[\"olfactometer_valves\"][olf_id]\n", "            if df is None or getattr(df, \"empty\", True):\n", "                continue\n", "            s = df[col].astype(bool)\n", "            rises = s & ~s.shift(1, fill_value=False)\n", "            falls = ~s & s.shift(1, fill_value=False)\n", "            starts = list(s.index[rises])\n", "            ends = list(s.index[falls])\n", "            i = j = 0\n", "            while i < len(starts) and j < len(ends):\n", "                if ends[j] <= starts[i]:\n", "                    j += 1\n", "                    continue\n", "                dur_ms = (ends[j] - starts[i]).total_seconds() * 1000.0\n", "                evs.append({\n", "                    \"start\": starts[i],\n", "                    \"end\": ends[j],\n", "                    \"duration_ms\": dur_ms,\n", "                    \"olf_id\": olf_id,\n", "                    \"idx\": idx,\n", "                    \"col\": col,\n", "                })\n", "                i += 1\n", "                j += 1\n", "        evs.sort(key=lambda e: e[\"start\"])\n", "        return evs\n", "\n", "    # --- Build completed trials dataframe and inter-odor windows ---\n", "    cls = trial_outcomes_complete.get(\"classification\", trial_outcomes_complete)\n", "    completed_keys = [\n", "        \"completed_sequence_rewarded\",\n", "        \"completed_sequence_unrewarded\",\n", "        \"completed_sequence_reward_timeout\",\n", "    ]\n", "    completed_dfs = [cls[k] for k in completed_keys if k in cls and isinstance(cls[k], pd.DataFrame)]\n", "    completed_df = pd.concat(completed_dfs, ignore_index=True) if completed_dfs else pd.DataFrame()\n", "\n", "    def _trial_id(row):\n", "        for k in [\"trial_id\", \"trial_index\", \"sequence_index\", \"Trial\", \"Sequence\", \"trial\"]:\n", "            if k in row and pd.notna(row[k]):\n", "                return row[k]\n", "        return row.name\n", "\n", "    # Collect inter-odor windows per trial: [(trial_id, pos_i, pos_j, win_start, win_end)]\n", "    windows = []\n", "    for _, row in completed_df.iterrows():\n", "        pov = row.get(\"position_valve_times\")\n", "        if not isinstance(pov, dict) or not pov:\n", "            continue\n", "        tid = _trial_id(row)\n", "        # sort positions by position number\n", "        positions = sorted([p for p in pov.keys() if isinstance(p, (int, np.integer))])\n", "        # windows between consecutive positions\n", "        for i in range(len(positions) - 1):\n", "            p_i = positions[i]\n", "            p_j = positions[i + 1]\n", "            end_i = pov[p_i].get(\"valve_end\")\n", "            start_j = pov[p_j].get(\"valve_start\")\n", "            if pd.isna(end_i) or pd.isna(start_j) or end_i is None or start_j is None:\n", "                continue\n", "            if end_i >= start_j:\n", "                # overlapping/invalid; skip\n", "                continue\n", "            win_start = end_i - pd.Timedelta(milliseconds=onset_slack_ms)\n", "            win_end = start_j + pd.Timedelta(milliseconds=onset_slack_ms)\n", "            windows.append((tid, p_i, p_j, win_start, win_end))\n", "\n", "    # --- Scan purge events and match onsets within windows ---\n", "    purge_events = _purge_events(odor_map)\n", "    starts = [e[\"start\"] for e in purge_events]\n", "\n", "    matches = []\n", "    for tid, p_i, p_j, ws, we in windows:\n", "        lo = bisect_left(starts, ws)\n", "        hi = bisect_right(starts, we)\n", "        for k in range(lo, hi):\n", "            e = purge_events[k]\n", "            if e[\"duration_ms\"] < threshold_ms:\n", "                matches.append({\n", "                    \"trial_id\": tid,\n", "                    \"from_pos\": p_i,\n", "                    \"to_pos\": p_j,\n", "                    \"start\": e[\"start\"],\n", "                    \"end\": e[\"end\"],\n", "                    \"duration_ms\": e[\"duration_ms\"],\n", "                    \"olf_id\": e[\"olf_id\"],\n", "                    \"col\": e[\"col\"],\n", "                })\n", "\n", "    # Summary/print\n", "    if verbose:\n", "        print(f\"Short Purge events (< {threshold_ms} ms) between odors in completed trials \"\n", "              f\"(onset slack \u00b1{onset_slack_ms} ms): {len(matches)}\")\n", "        by_trial = {}\n", "        for m in matches:\n", "            by_trial.setdefault(m[\"trial_id\"], 0)\n", "            by_trial[m[\"trial_id\"]] += 1\n", "        if matches:\n", "            for m in sorted(matches, key=lambda x: (x[\"trial_id\"], x[\"start\"])):\n", "                print(f\"- trial {m['trial_id']} pos {m['from_pos']}->{m['to_pos']}: \"\n", "                      f\"{m['start'].isoformat()} -> {m['end'].isoformat()} \"\n", "                      f\"({m['duration_ms']:.1f} ms)  olf {m['olf_id']} col '{m['col']}'\")\n", "            print(\"\\nCounts by trial:\")\n", "            for tid in sorted(by_trial):\n", "                print(f\"  trial {tid}: {by_trial[tid]}\")\n", "\n", "    return matches\n", "\n", "# Example usage:\n", "matches = list_short_purge_between_odors(trial_outcomes_complete, odor_map, threshold_ms=200.0, onset_slack_ms=100.0, verbose=True)\n", "len(matches)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# For a schema file, check any poke that is < threshold (e.g. 200 ms), within completed sequences\n", "import pandas as pd\n", "import json, ast\n", "from pathlib import Path\n", "from collections.abc import Mapping\n", "\n", "def _parse_obj(x):\n", "    if isinstance(x, (dict, list, tuple)):\n", "        return x\n", "    if x is None or (isinstance(x, float) and pd.isna(x)):\n", "        return None\n", "    if isinstance(x, str):\n", "        s = x.strip()\n", "        if not s:\n", "            return None\n", "        # Try JSON first, then Python literal\n", "        try:\n", "            return json.loads(s)\n", "        except Exception:\n", "            try:\n", "                return ast.literal_eval(s)\n", "            except Exception:\n", "                return None\n", "    return None\n", "\n", "def _iter_pos_items(ppt):\n", "    # Yield (position, info_dict) for dict/list\n", "    if isinstance(ppt, Mapping):\n", "        for k, v in ppt.items():\n", "            if not isinstance(v, Mapping):\n", "                try:\n", "                    v = dict(v)\n", "                except Exception:\n", "                    continue\n", "            pos = v.get(\"position\")\n", "            if pos is None:\n", "                try:\n", "                    pos = int(k)\n", "                except Exception:\n", "                    pos = k\n", "            yield pos, v\n", "    elif isinstance(ppt, (list, tuple)):\n", "        for v in ppt:\n", "            if isinstance(v, Mapping):\n", "                yield v.get(\"position\"), v\n", "\n", "def _normalize_valves(pvt):\n", "    # Normalize to dict: position -> valve dict\n", "    out = {}\n", "    if isinstance(pvt, Mapping):\n", "        items = list(pvt.items())\n", "    elif isinstance(pvt, (list, tuple)):\n", "        items = [(v.get(\"position\"), v) for v in pvt if isinstance(v, Mapping)]\n", "    else:\n", "        items = []\n", "    for k, v in items:\n", "        if not isinstance(v, Mapping):\n", "            try:\n", "                v = dict(v)\n", "            except Exception:\n", "                v = {}\n", "        pos = v.get(\"position\")\n", "        if pos is None:\n", "            try:\n", "                pos = int(k)\n", "            except Exception:\n", "                pos = k\n", "        out[pos] = v\n", "    return out\n", "\n", "def extract_short_pokes_from_saved(schema_path: str | Path, threshold_ms: float = 200.0) -> pd.DataFrame:\n", "    schema_path = Path(schema_path)\n", "    csv_path = schema_path.with_suffix(\"\").with_suffix(\".csv\")  # replace .schema.json -> .csv\n", "    if not csv_path.exists():\n", "        # fallback: try sibling CSV with same stem\n", "        csv_path = schema_path.parent / (schema_path.stem.replace(\".schema\", \"\") + \".csv\")\n", "    if not csv_path.exists():\n", "        raise FileNotFoundError(f\"CSV not found next to schema: {csv_path}\")\n", "\n", "    df = pd.read_csv(csv_path)\n", "\n", "    # Identify column names for per-position fields\n", "    poke_col = None\n", "    valve_col = None\n", "    for c in df.columns:\n", "        lc = c.lower()\n", "        if poke_col is None and \"position\" in lc and \"poke\" in lc:\n", "            poke_col = c\n", "        if valve_col is None and \"position\" in lc and \"valve\" in lc:\n", "            valve_col = c\n", "    if poke_col is None:\n", "        raise KeyError(\"Could not find position_poke_times column in CSV\")\n", "    if valve_col is None:\n", "        # some outputs may not store per-position valves; still proceed\n", "        valve_col = None\n", "\n", "    rows = []\n", "    for _, row in df.iterrows():\n", "        ppt = _parse_obj(row.get(poke_col))\n", "        if ppt is None:\n", "            continue\n", "        pvt_raw = _parse_obj(row.get(valve_col)) if valve_col else None\n", "        valve_map = _normalize_valves(pvt_raw) if pvt_raw is not None else {}\n", "\n", "        run_id = row.get(\"run_id\")\n", "        trial_id = row.get(\"trial_id\")\n", "\n", "        for pos, info in _iter_pos_items(ppt):\n", "            if not isinstance(info, Mapping):\n", "                try:\n", "                    info = dict(info)\n", "                except Exception:\n", "                    continue\n", "            poke_ms = pd.to_numeric(info.get(\"poke_time_ms\"), errors=\"coerce\")\n", "            if pd.isna(poke_ms) or poke_ms <= 0 or poke_ms >= threshold_ms:\n", "                continue\n", "\n", "            # normalize pos\n", "            try:\n", "                pos_norm = int(pos) if pos is not None else None\n", "            except Exception:\n", "                pos_norm = pos\n", "\n", "            vt = valve_map.get(pos_norm, {})\n", "            odor = info.get(\"odor_name\") or (vt or {}).get(\"odor_name\")\n", "            first_in = info.get(\"poke_first_in\")\n", "            valve_open = (vt or {}).get(\"valve_open_ts\")\n", "            valve_close = (vt or {}).get(\"valve_close_ts\")\n", "            event_ts = first_in if first_in is not None else valve_open  # measurement start\n", "\n", "            rows.append({\n", "                \"run_id\": run_id,\n", "                \"trial_id\": trial_id,\n", "                \"position\": pos_norm,\n", "                \"odor\": odor,\n", "                \"poke_ms\": float(poke_ms),\n", "                \"event_ts\": event_ts,\n", "                \"valve_open_ts\": valve_open,\n", "                \"valve_close_ts\": valve_close,\n", "                \"poke_first_in\": first_in,\n", "            })\n", "\n", "    out = pd.DataFrame(rows)\n", "    if not out.empty:\n", "        out[\"event_ts\"] = pd.to_datetime(out[\"event_ts\"], errors=\"coerce\")\n", "        out = out.sort_values([\"run_id\",\"trial_id\",\"position\",\"event_ts\"], kind=\"stable\", na_position=\"last\").reset_index(drop=True)\n", "    return out\n", "\n", "# Use your saved path\n", "schema_path = \"/Volumes/harris/hypnose/derivatives/sub-040_id-259/ses-022_date-20251125/saved_analysis_results/completed_sequences_with_response_times.schema.json\"\n", "short_pokes = extract_short_pokes_from_saved(schema_path, threshold_ms=200.0)\n", "display(short_pokes)\n", "print(f\"{len(short_pokes)} positions with poke_time_ms < 200 ms\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Identify trial IDs for trials with poke time bewlow threshold \n", "threshold_ms = 200.0\n", "\n", "# Get the classification DataFrames\n", "cls = trial_outcomes_complete.get(\"classification\", trial_outcomes_complete)\n", "\n", "completed_keys = [\n", "    \"completed_sequence_rewarded\",\n", "    \"completed_sequence_unrewarded\",\n", "    \"completed_sequence_reward_timeout\",\n", "]\n", "completed_dfs = [cls[k] for k in completed_keys if k in cls and isinstance(cls[k], pd.DataFrame)]\n", "completed_df = pd.concat(completed_dfs, ignore_index=True) if completed_dfs else pd.DataFrame()\n", "\n", "def _trial_id(row):\n", "    for k in [\"trial_id\", \"trial_index\", \"sequence_index\", \"Trial\", \"Sequence\", \"trial\"]:\n", "        if k in row and pd.notna(row[k]):\n", "            return row[k]\n", "    return row.name  # fallback\n", "\n", "hits = []\n", "for _, row in completed_df.iterrows():\n", "    pos_pokes = row.get(\"position_poke_times\")\n", "    if not isinstance(pos_pokes, dict):\n", "        continue\n", "    tid = _trial_id(row)\n", "    for pos, info in pos_pokes.items():\n", "        if not isinstance(info, dict):\n", "            continue\n", "        ms = info.get(\"poke_time_ms\")\n", "        if ms is None:\n", "            continue\n", "        if ms < threshold_ms:\n", "            odor = info.get(\"odor_name\")\n", "            hits.append((ms, tid, pos, odor))\n", "\n", "# Print results sorted by poke time\n", "if not hits:\n", "    print(f\"No completed-trial positions with poke_time_ms < {threshold_ms} ms found.\")\n", "else:\n", "    hits.sort(key=lambda x: x[0])\n", "    print(f\"Trials with poke_time_ms < {threshold_ms} ms (n={len(hits)}):\")\n", "    for ms, tid, pos, odor in hits:\n", "        print(f\"- trial {tid}, position {pos}, {odor}: {ms:.1f} ms\")"]}], "metadata": {"kernelspec": {"display_name": "hypnose", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.11"}}, "nbformat": 4, "nbformat_minor": 2}